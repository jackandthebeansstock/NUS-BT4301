{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed56f227-e204-4182-941d-1e4fcd3da55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76be38d-ce79-420f-8370-3f79cd99d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
      "     ---------------------------------------- 0.0/317.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 2.1/317.2 MB 10.7 MB/s eta 0:00:30\n",
      "      -------------------------------------- 4.7/317.2 MB 11.4 MB/s eta 0:00:28\n",
      "      -------------------------------------- 6.3/317.2 MB 11.7 MB/s eta 0:00:27\n",
      "     - ------------------------------------- 8.9/317.2 MB 10.6 MB/s eta 0:00:29\n",
      "     - ------------------------------------ 11.3/317.2 MB 10.8 MB/s eta 0:00:29\n",
      "     - ------------------------------------ 13.9/317.2 MB 11.0 MB/s eta 0:00:28\n",
      "     - ------------------------------------ 16.3/317.2 MB 11.2 MB/s eta 0:00:27\n",
      "     -- ----------------------------------- 18.9/317.2 MB 11.3 MB/s eta 0:00:27\n",
      "     -- ----------------------------------- 21.5/317.2 MB 11.4 MB/s eta 0:00:26\n",
      "     -- ----------------------------------- 24.1/317.2 MB 11.5 MB/s eta 0:00:26\n",
      "     --- ---------------------------------- 26.5/317.2 MB 11.5 MB/s eta 0:00:26\n",
      "     --- ---------------------------------- 29.1/317.2 MB 11.5 MB/s eta 0:00:25\n",
      "     --- ---------------------------------- 31.5/317.2 MB 11.5 MB/s eta 0:00:25\n",
      "     ---- --------------------------------- 34.1/317.2 MB 11.6 MB/s eta 0:00:25\n",
      "     ---- --------------------------------- 36.7/317.2 MB 11.6 MB/s eta 0:00:25\n",
      "     ---- --------------------------------- 39.3/317.2 MB 11.6 MB/s eta 0:00:24\n",
      "     ---- --------------------------------- 41.7/317.2 MB 11.6 MB/s eta 0:00:24\n",
      "     ----- -------------------------------- 44.3/317.2 MB 11.6 MB/s eta 0:00:24\n",
      "     ----- -------------------------------- 46.9/317.2 MB 11.6 MB/s eta 0:00:24\n",
      "     ----- -------------------------------- 49.3/317.2 MB 11.7 MB/s eta 0:00:23\n",
      "     ------ ------------------------------- 51.9/317.2 MB 11.7 MB/s eta 0:00:23\n",
      "     ------ ------------------------------- 54.3/317.2 MB 11.7 MB/s eta 0:00:23\n",
      "     ------ ------------------------------- 56.9/317.2 MB 11.7 MB/s eta 0:00:23\n",
      "     ------- ------------------------------ 59.5/317.2 MB 11.7 MB/s eta 0:00:23\n",
      "     ------- ------------------------------ 61.9/317.2 MB 11.7 MB/s eta 0:00:22\n",
      "     ------- ------------------------------ 64.5/317.2 MB 11.7 MB/s eta 0:00:22\n",
      "     -------- ----------------------------- 67.1/317.2 MB 11.7 MB/s eta 0:00:22\n",
      "     -------- ----------------------------- 69.5/317.2 MB 11.7 MB/s eta 0:00:22\n",
      "     -------- ----------------------------- 72.1/317.2 MB 11.8 MB/s eta 0:00:21\n",
      "     -------- ----------------------------- 74.2/317.2 MB 11.7 MB/s eta 0:00:21\n",
      "     --------- ---------------------------- 76.8/317.2 MB 11.7 MB/s eta 0:00:21\n",
      "     --------- ---------------------------- 79.4/317.2 MB 11.7 MB/s eta 0:00:21\n",
      "     --------- ---------------------------- 81.8/317.2 MB 11.7 MB/s eta 0:00:21\n",
      "     ---------- --------------------------- 84.1/317.2 MB 11.7 MB/s eta 0:00:20\n",
      "     ---------- --------------------------- 86.8/317.2 MB 11.7 MB/s eta 0:00:20\n",
      "     ---------- --------------------------- 89.4/317.2 MB 11.7 MB/s eta 0:00:20\n",
      "     ---------- --------------------------- 91.8/317.2 MB 11.7 MB/s eta 0:00:20\n",
      "     ----------- -------------------------- 94.1/317.2 MB 11.7 MB/s eta 0:00:20\n",
      "     ----------- -------------------------- 96.5/317.2 MB 11.7 MB/s eta 0:00:19\n",
      "     ----------- -------------------------- 98.8/317.2 MB 11.7 MB/s eta 0:00:19\n",
      "     ----------- ------------------------- 101.2/317.2 MB 11.7 MB/s eta 0:00:19\n",
      "     ------------ ------------------------ 103.3/317.2 MB 11.6 MB/s eta 0:00:19\n",
      "     ------------ ------------------------ 105.4/317.2 MB 11.6 MB/s eta 0:00:19\n",
      "     ------------ ------------------------ 107.2/317.2 MB 11.5 MB/s eta 0:00:19\n",
      "     ------------ ------------------------ 109.6/317.2 MB 11.5 MB/s eta 0:00:19\n",
      "     ------------- ----------------------- 111.9/317.2 MB 11.5 MB/s eta 0:00:18\n",
      "     ------------- ----------------------- 114.6/317.2 MB 11.5 MB/s eta 0:00:18\n",
      "     ------------- ----------------------- 116.9/317.2 MB 11.5 MB/s eta 0:00:18\n",
      "     ------------- ----------------------- 119.3/317.2 MB 11.5 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 121.6/317.2 MB 11.5 MB/s eta 0:00:17\n",
      "     -------------- ---------------------- 123.7/317.2 MB 11.5 MB/s eta 0:00:17\n",
      "     -------------- ---------------------- 125.8/317.2 MB 11.4 MB/s eta 0:00:17\n",
      "     -------------- ---------------------- 128.2/317.2 MB 11.4 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 130.3/317.2 MB 11.4 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 132.6/317.2 MB 11.4 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 135.3/317.2 MB 11.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 137.9/317.2 MB 11.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 140.2/317.2 MB 11.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 142.9/317.2 MB 11.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 145.2/317.2 MB 11.5 MB/s eta 0:00:16\n",
      "     ----------------- ------------------- 147.8/317.2 MB 11.5 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 150.5/317.2 MB 11.5 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 152.8/317.2 MB 11.5 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 155.5/317.2 MB 11.5 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 158.1/317.2 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------ ------------------ 160.7/317.2 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 163.1/317.2 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 165.4/317.2 MB 11.5 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 168.0/317.2 MB 11.5 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 170.4/317.2 MB 11.5 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 173.0/317.2 MB 11.5 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 175.4/317.2 MB 11.5 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 178.0/317.2 MB 11.5 MB/s eta 0:00:13\n",
      "     --------------------- --------------- 180.6/317.2 MB 11.5 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 183.2/317.2 MB 11.5 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 185.6/317.2 MB 11.5 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 188.2/317.2 MB 11.6 MB/s eta 0:00:12\n",
      "     ---------------------- -------------- 190.8/317.2 MB 11.6 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 193.2/317.2 MB 11.6 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 195.8/317.2 MB 11.6 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 198.4/317.2 MB 11.6 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 201.1/317.2 MB 11.6 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 203.4/317.2 MB 11.6 MB/s eta 0:00:10\n",
      "     ------------------------ ------------ 206.0/317.2 MB 11.6 MB/s eta 0:00:10\n",
      "     ------------------------ ------------ 208.4/317.2 MB 11.6 MB/s eta 0:00:10\n",
      "     ------------------------ ------------ 211.0/317.2 MB 11.6 MB/s eta 0:00:10\n",
      "     ------------------------ ------------ 213.4/317.2 MB 11.6 MB/s eta 0:00:09\n",
      "     ------------------------- ----------- 216.0/317.2 MB 11.6 MB/s eta 0:00:09\n",
      "     ------------------------- ----------- 218.6/317.2 MB 11.6 MB/s eta 0:00:09\n",
      "     ------------------------- ----------- 221.2/317.2 MB 11.6 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 223.6/317.2 MB 11.6 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 226.2/317.2 MB 11.6 MB/s eta 0:00:08\n",
      "     -------------------------- ---------- 228.9/317.2 MB 11.6 MB/s eta 0:00:08\n",
      "     -------------------------- ---------- 231.5/317.2 MB 11.6 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 233.8/317.2 MB 11.6 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 236.5/317.2 MB 11.6 MB/s eta 0:00:07\n",
      "     --------------------------- --------- 239.1/317.2 MB 11.6 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 241.4/317.2 MB 11.6 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 243.8/317.2 MB 11.6 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 246.4/317.2 MB 11.6 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 249.0/317.2 MB 11.6 MB/s eta 0:00:06\n",
      "     ----------------------------- ------- 251.7/317.2 MB 11.7 MB/s eta 0:00:06\n",
      "     ----------------------------- ------- 254.0/317.2 MB 11.6 MB/s eta 0:00:06\n",
      "     ----------------------------- ------- 256.6/317.2 MB 11.7 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 259.0/317.2 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 261.6/317.2 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 264.0/317.2 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 266.6/317.2 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 269.2/317.2 MB 11.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 271.6/317.2 MB 11.7 MB/s eta 0:00:04\n",
      "     ------------------------------- ----- 274.2/317.2 MB 11.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 276.8/317.2 MB 11.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 279.2/317.2 MB 11.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 281.8/317.2 MB 11.7 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 284.4/317.2 MB 11.7 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 286.8/317.2 MB 11.7 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 289.4/317.2 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 291.8/317.2 MB 11.7 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 294.6/317.2 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 297.0/317.2 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 299.6/317.2 MB 11.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 302.3/317.2 MB 11.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 304.9/317.2 MB 11.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 307.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  309.9/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  312.5/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  314.8/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.2 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- 317.2/317.2 MB 11.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747900 sha256=23765d4db52b0c20310792ce0c9f846d368e08c084a66efb8aa57e120612c5ee\n",
      "  Stored in directory: c:\\users\\cephe\\appdata\\local\\pip\\cache\\wheels\\8f\\cb\\c0\\cc57eb1bf0f9dc87cdaf2b0dbac49e58a210ff68d21d6fc709\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7effc5b5-5acc-4670-a529-ed8d27d1a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb3ae9-a7a5-47c6-9f05-fba46e264204",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_product = \"meta_Kindle_store.parquet\"\n",
    "df_p = pl.read_parquet(file_path_product)\n",
    "try:\n",
    "    df_p = pl.read_parquet(file_path_product)\n",
    "    print(df_p) #or df_p.head(), df_p.tail(), df_p.to_string()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path_product}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab730c2-69b8-4294-9dc6-363124be7936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>rating</th><th>title</th><th>text</th><th>images</th><th>asin</th><th>parent_asin</th><th>user_id</th><th>timestamp</th><th>helpful_vote</th><th>verified_purchase</th></tr><tr><td>f64</td><td>str</td><td>str</td><td>list[struct[4]]</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>bool</td></tr></thead><tbody><tr><td>5.0</td><td>&quot;excellent writer reminds me of…</td><td>&quot;GRUMLEY is on par with Clive C…</td><td>[]</td><td>&quot;B00LXRJICK&quot;</td><td>&quot;B00LXRJICK&quot;</td><td>&quot;AFKZENTNBQ7A7V7UXW5JJI6UGRYQ&quot;</td><td>1427541413000</td><td>0</td><td>false</td></tr><tr><td>3.0</td><td>&quot;Alright book&quot;</td><td>&quot;The book Fade was not my favor…</td><td>[]</td><td>&quot;B073DFP8VC&quot;</td><td>&quot;B073DFP8VC&quot;</td><td>&quot;AHGTHCERTEZUXNBLJ5SWHK2CDLXA&quot;</td><td>1504226946142</td><td>0</td><td>true</td></tr><tr><td>5.0</td><td>&quot;Hats off to Fern Michaels for …</td><td>&quot;I have been a fan of this auth…</td><td>[]</td><td>&quot;B07QVH25KX&quot;</td><td>&quot;B07QVH25KX&quot;</td><td>&quot;AHFY2QSS6PK5MHSYZFI6TXUYNPLQ&quot;</td><td>1644883955777</td><td>0</td><td>true</td></tr><tr><td>5.0</td><td>&quot;Great read&quot;</td><td>&quot;This book is more than just ab…</td><td>[]</td><td>&quot;B004Y1NWQU&quot;</td><td>&quot;B004Y1NWQU&quot;</td><td>&quot;AHFY2QSS6PK5MHSYZFI6TXUYNPLQ&quot;</td><td>1363027885000</td><td>0</td><td>false</td></tr><tr><td>5.0</td><td>&quot;Add to legend f Arthur&quot;</td><td>&quot;Good twist on the ledgen of Ki…</td><td>[]</td><td>&quot;B08M993CNC&quot;</td><td>&quot;B08M993CNC&quot;</td><td>&quot;AFWHJ6O3PV4JC7PVOJH6CPULO2KQ&quot;</td><td>1637557512064</td><td>0</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌────────┬───────────────┬──────────────┬──────────────┬───┬──────────────┬──────────────┬──────────────┬──────────────┐\n",
       "│ rating ┆ title         ┆ text         ┆ images       ┆ … ┆ user_id      ┆ timestamp    ┆ helpful_vote ┆ verified_pur │\n",
       "│ ---    ┆ ---           ┆ ---          ┆ ---          ┆   ┆ ---          ┆ ---          ┆ ---          ┆ chase        │\n",
       "│ f64    ┆ str           ┆ str          ┆ list[struct[ ┆   ┆ str          ┆ i64          ┆ i64          ┆ ---          │\n",
       "│        ┆               ┆              ┆ 4]]          ┆   ┆              ┆              ┆              ┆ bool         │\n",
       "╞════════╪═══════════════╪══════════════╪══════════════╪═══╪══════════════╪══════════════╪══════════════╪══════════════╡\n",
       "│ 5.0    ┆ excellent     ┆ GRUMLEY is   ┆ []           ┆ … ┆ AFKZENTNBQ7A ┆ 142754141300 ┆ 0            ┆ false        │\n",
       "│        ┆ writer        ┆ on par with  ┆              ┆   ┆ 7V7UXW5JJI6U ┆ 0            ┆              ┆              │\n",
       "│        ┆ reminds me    ┆ Clive C…     ┆              ┆   ┆ GRYQ         ┆              ┆              ┆              │\n",
       "│        ┆ of…           ┆              ┆              ┆   ┆              ┆              ┆              ┆              │\n",
       "│ 3.0    ┆ Alright book  ┆ The book     ┆ []           ┆ … ┆ AHGTHCERTEZU ┆ 150422694614 ┆ 0            ┆ true         │\n",
       "│        ┆               ┆ Fade was not ┆              ┆   ┆ XNBLJ5SWHK2C ┆ 2            ┆              ┆              │\n",
       "│        ┆               ┆ my favor…    ┆              ┆   ┆ DLXA         ┆              ┆              ┆              │\n",
       "│ 5.0    ┆ Hats off to   ┆ I have been  ┆ []           ┆ … ┆ AHFY2QSS6PK5 ┆ 164488395577 ┆ 0            ┆ true         │\n",
       "│        ┆ Fern Michaels ┆ a fan of     ┆              ┆   ┆ MHSYZFI6TXUY ┆ 7            ┆              ┆              │\n",
       "│        ┆ for …         ┆ this auth…   ┆              ┆   ┆ NPLQ         ┆              ┆              ┆              │\n",
       "│ 5.0    ┆ Great read    ┆ This book is ┆ []           ┆ … ┆ AHFY2QSS6PK5 ┆ 136302788500 ┆ 0            ┆ false        │\n",
       "│        ┆               ┆ more than    ┆              ┆   ┆ MHSYZFI6TXUY ┆ 0            ┆              ┆              │\n",
       "│        ┆               ┆ just ab…     ┆              ┆   ┆ NPLQ         ┆              ┆              ┆              │\n",
       "│ 5.0    ┆ Add to legend ┆ Good twist   ┆ []           ┆ … ┆ AFWHJ6O3PV4J ┆ 163755751206 ┆ 0            ┆ true         │\n",
       "│        ┆ f Arthur      ┆ on the       ┆              ┆   ┆ C7PVOJH6CPUL ┆ 4            ┆              ┆              │\n",
       "│        ┆               ┆ ledgen of    ┆              ┆   ┆ O2KQ         ┆              ┆              ┆              │\n",
       "│        ┆               ┆ Ki…          ┆              ┆   ┆              ┆              ┆              ┆              │\n",
       "└────────┴───────────────┴──────────────┴──────────────┴───┴──────────────┴──────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f20517-d926-4248-bf58-c10c59cbb9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c74f398-8eb4-4c20-b85c-4442f6f4a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_customer = \"Kindle_store.parquet\"\n",
    "try:\n",
    "    df_c = pl.read_parquet(file_path_customer)\n",
    "    df_c.head()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path_customer}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e14456a-6747-4fec-973a-f15ede2c6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "Total items before filtering: 50000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pandas script for preprocessing the customer.xlsx file.\n",
    "It performs the following:\n",
    "    - Reads the Excel file.\n",
    "    - Drops the 'images' column if present.\n",
    "    - Casts selected columns to appropriate types.\n",
    "    - Computes a new \"year\" column by converting the original timestamp (in ms) to a year.\n",
    "    - Scales the timestamp from milliseconds to days.\n",
    "    - Computes counts per user_id and parent_asin.\n",
    "    - Removes rows where either count is less than 3.\n",
    "    - Prints removal statistics and the number of items per year.\n",
    "    - Writes the cleaned DataFrame with extra count columns to a new Excel file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_customer(input_path=\"customer.xlsx\", output_path=\"customer_clean.xlsx\"):\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    print(df.size)\n",
    "    if \"images\" in df.columns:\n",
    "        df = df.drop(columns=[\"images\"])\n",
    "    df[\"rating\"] = df[\"rating\"].astype(int)\n",
    "    df[\"title\"] = df[\"title\"].astype(str)\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"asin\"] = df[\"asin\"].astype(str)\n",
    "    df[\"parent_asin\"] = df[\"parent_asin\"].astype(str)\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    #df[\"timestamp\"] = df[\"timestamp\"].astype(int)\n",
    "    df[\"helpful_vote\"] = df[\"helpful_vote\"].astype(int)\n",
    "    df[\"verified_purchase\"] = df[\"verified_purchase\"].astype(bool)\n",
    "    # Compute counts per user_id and parent_asin.\n",
    "    df[\"count_user_id\"] = df.groupby(\"user_id\")[\"user_id\"].transform(\"count\")\n",
    "    df[\"count_parent_asin\"] = df.groupby(\"parent_asin\")[\"parent_asin\"].transform(\"count\")\n",
    "    \n",
    "    total_items = df.shape[0]\n",
    "    \n",
    "    print(\"Total items before filtering:\", total_items)\n",
    "    \n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_customer(\"customer.xlsx\", \"customer_clean.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546f2b77-b658-4c50-84cf-0e8ac2cd6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items per year:\n",
      "Year 1969: 10020939 items\n",
      "Year 1970: 10014420 items\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import the cleaned Kindle Store dataset.\n",
    "df_clean = pd.read_parquet(\"Kindle_Store_clean.parquet\", engine=\"pyarrow\")\n",
    "print(\"Number of items per year:\")\n",
    "df_clean[\"year\"] = pd.to_datetime(df_clean[\"timestamp\"] * 86400, unit=\"s\").dt.year\n",
    "year_counts = df_clean.groupby(\"year\").size().sort_index()\n",
    "for yr, count in year_counts.items():\n",
    "    print(f\"Year {yr}: {count} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aee0f2b-ef1a-436a-8cd7-7d836e2761ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pandas script for preprocessing the product.xlsx file.\n",
    "It performs the following:\n",
    "    - Reads the Excel file using openpyxl.\n",
    "    - Drops unnecessary columns: main_category, images, videos, store, features, description.\n",
    "    - Extracts the author’s name from the JSON in the author column.\n",
    "    - Cleans the categories column by removing 'Kindle Store' and 'Kindle eBooks'.\n",
    "    - From the details JSON, extracts:\n",
    "         - 'Publication date': converts a date like \"May 1, 2013\" into Unix epoch days (float).\n",
    "         - 'Print length': extracts the number from a string like \"278 pages\" and converts to int.\n",
    "         - 'Language': extracts the language.\n",
    "    - Casts the remaining columns to the proper types.\n",
    "    - Writes the cleaned DataFrame to a new Excel file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "def extract_author_name(author):\n",
    "    \"\"\"\n",
    "    Given a JSON-like author value (as a dict or string), extracts the 'name' field.\n",
    "    Uses ast.literal_eval if the value is a string.\n",
    "    \"\"\"\n",
    "    if isinstance(author, dict):\n",
    "        return author.get(\"name\", None)\n",
    "    if isinstance(author, str):\n",
    "        try:\n",
    "            # Convert string representation of dict to a Python dict.\n",
    "            author_dict = ast.literal_eval(author)\n",
    "            return author_dict.get(\"name\", None)\n",
    "        except Exception:\n",
    "            return author\n",
    "    return author\n",
    "\n",
    "def clean_categories(categories):\n",
    "    \"\"\"\n",
    "    Removes 'Kindle Store' and 'Kindle eBooks' from the categories.\n",
    "    If categories is a list, returns a list without the unwanted items.\n",
    "    If it is a string, splits on commas, filters, and rejoins.\n",
    "    \"\"\"\n",
    "    unwanted = {\"Kindle Store\", \"Kindle eBooks\"}\n",
    "    if isinstance(categories, list):\n",
    "        return [cat for cat in categories if cat not in unwanted]\n",
    "    if isinstance(categories, str):\n",
    "        cats = [cat.strip() for cat in categories.split(\",\")]\n",
    "        filtered = [cat for cat in cats if cat not in unwanted]\n",
    "        return \", \".join(filtered)\n",
    "    return categories\n",
    "\n",
    "def extract_details(details):\n",
    "    \"\"\"\n",
    "    Given a JSON-like details value (as a dict or string), extracts:\n",
    "      - Publication date: parses a date like \"May 1, 2013\" and converts it to Unix epoch days.\n",
    "      - Print length: extracts numeric part from a string like \"278 pages\".\n",
    "      - Language: extracts the language.\n",
    "    Uses ast.literal_eval if needed.\n",
    "    \"\"\"\n",
    "    pub_date = None\n",
    "    print_length_str = None\n",
    "    language = None\n",
    "    \n",
    "    if isinstance(details, dict):\n",
    "        pub_date = details.get(\"Publication date\", None)\n",
    "        print_length_str = details.get(\"Print length\", None)\n",
    "        language = details.get(\"Language\", None)\n",
    "    elif isinstance(details, str):\n",
    "        try:\n",
    "            details_dict = ast.literal_eval(details)\n",
    "            pub_date = details_dict.get(\"Publication date\", None)\n",
    "            print_length_str = details_dict.get(\"Print length\", None)\n",
    "            language = details_dict.get(\"Language\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Convert publication date to Unix epoch days.\n",
    "    pub_days = None\n",
    "    if pub_date:\n",
    "        try:\n",
    "            # Try a known format e.g., \"May 1, 2013\"\n",
    "            dt = datetime.strptime(pub_date, \"%B %d, %Y\")\n",
    "            pub_days = dt.timestamp() / 86400\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Fall back to pandas conversion.\n",
    "                dt = pd.to_datetime(pub_date)\n",
    "                pub_days = dt.timestamp() / 86400\n",
    "            except Exception:\n",
    "                pub_days = None\n",
    "\n",
    "    # Extract print length from a string like \"278 pages\".\n",
    "    print_length = None\n",
    "    if print_length_str:\n",
    "        digits = ''.join(filter(str.isdigit, str(print_length_str)))\n",
    "        if digits:\n",
    "            print_length = int(digits)\n",
    "    \n",
    "    return pub_days, print_length, language\n",
    "\n",
    "def preprocess_product(input_path=\"product.xlsx\", output_path=\"product_clean.xlsx\"):\n",
    "    # Read the Excel file using openpyxl.\n",
    "    df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    \n",
    "    # Drop unnecessary columns.\n",
    "    cols_to_drop = [\"main_category\", \"images\", \"videos\", \"store\", \"features\", \"description\"]\n",
    "    df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Process the author column to extract the 'name' from the JSON.\n",
    "    df[\"author\"] = df[\"author\"].apply(extract_author_name)\n",
    "    \n",
    "    # Clean the categories column.\n",
    "    df[\"categories\"] = df[\"categories\"].apply(clean_categories)\n",
    "    df[\"categories\"] = df[\"categories\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Extract details from the details column.\n",
    "    details_extracted = df[\"details\"].apply(extract_details)\n",
    "    df[[\"publication_date\", \"print_length\", \"language\"]] = pd.DataFrame(details_extracted.tolist(), index=df.index)\n",
    "    \n",
    "    # Cast remaining columns.\n",
    "    df[\"title\"] = df[\"title\"].astype(str)\n",
    "    df[\"subtitle\"] = df[\"subtitle\"].astype(str)\n",
    "    df[\"average_rating\"] = df[\"average_rating\"].astype(float)\n",
    "    df[\"rating_number\"] = df[\"rating_number\"].astype(int)\n",
    "    df[\"price\"] = df[\"price\"].astype(float)\n",
    "    df[\"parent_asin\"] = df[\"parent_asin\"].astype(str)\n",
    "    \n",
    "    # Select final columns in the desired order.\n",
    "    final_columns = [\n",
    "        \"title\", \"subtitle\", \"author\", \"average_rating\", \"rating_number\", \"price\",\n",
    "        \"categories\", \"publication_date\", \"parent_asin\", \"language\", \"print_length\"\n",
    "    ]\n",
    "    df_clean = df[final_columns]\n",
    "    \n",
    "    df_clean.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "    return df_clean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_product(\"product.xlsx\", \"product_clean.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c05e421-cc34-4ae8-95a8-327da7e582b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50000 rows saved to product.xlsx\n",
      "First 50000 rows saved to customer.xlsx\n"
     ]
    }
   ],
   "source": [
    "MAX_ROWS_TO_SAVE = 50000\n",
    "\n",
    "def save_first_n_rows_to_excel(df: pl.DataFrame, output_file: str, n: int = MAX_ROWS_TO_SAVE):\n",
    "    \"\"\"\n",
    "    Retrieves the first 'n' rows of a Polars DataFrame and saves them to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        df: The Polars DataFrame.\n",
    "        output_file: The path to the Excel file to save.\n",
    "        n: The number of rows to select (default: MAX_ROWS_TO_SAVE).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        selected_rows = df.head(n) if df.height > n else df\n",
    "        pandas_df = selected_rows.to_pandas()\n",
    "        pandas_df.to_excel(output_file, index=False)\n",
    "        print(f\"First {min(n, df.height)} rows saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "try:\n",
    "    file_path_product = \"meta_Kindle_store.parquet\"\n",
    "    df_p = pl.read_parquet(file_path_product)\n",
    "    file_path_customer = \"Kindle_store.parquet\"\n",
    "    df_c = pl.read_parquet(file_path_customer)\n",
    "    save_first_n_rows_to_excel(df_p, \"product.xlsx\")\n",
    "    save_first_n_rows_to_excel(df_c, \"customer.xlsx\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found at the specified path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea70907f-8a98-48f1-bd3f-4891ae6f3b2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Create a Spark session\u001b[39;00m\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleSparkApp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Step 2: Create a simple DataFrame\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Let's create some sample data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m29\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJane Smith\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m35\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSam Brown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m22\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m--> 503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(\n\u001b[0;32m   1713\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1714\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART)\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleSparkApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Create a simple DataFrame\n",
    "# Let's create some sample data\n",
    "data = [(1, \"John Doe\", 29), (2, \"Jane Smith\", 35), (3, \"Sam Brown\", 22)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Convert the list of Row objects to a DataFrame\n",
    "df = spark.createDataFrame(data,schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# write the data into a csv file\n",
    "df.write.csv(\"output_data.csv\", header=True)\n",
    "\n",
    "# stop the spark application\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613910e-0b01-4a82-88df-6f4669ce4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def split_polars_dataframe(df: pl.DataFrame, num_files: int, output_dir: str, base_filename: str):\n",
    "    \"\"\"\n",
    "    Splits a Polars DataFrame into 'num_files' equal-sized files.\n",
    "\n",
    "    Args:\n",
    "        df: The Polars DataFrame to split.\n",
    "        num_files: The number of files to create.\n",
    "        output_dir: The directory to save the split files.\n",
    "        base_filename: The base filename for the split files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "        chunk_size = len(df) // num_files\n",
    "        remainder = len(df) % num_files\n",
    "\n",
    "        start_index = 0\n",
    "        for i in range(num_files):\n",
    "            end_index = start_index + chunk_size\n",
    "            if i < remainder:  # Distribute the remainder rows\n",
    "                end_index += 1\n",
    "\n",
    "            chunk = df[start_index:end_index]\n",
    "            output_file = os.path.join(output_dir, f\"{base_filename}_{i + 1}.parquet\")\n",
    "            chunk.write_parquet(output_file)\n",
    "            print(f\"Saved chunk {i + 1} to {output_file}\")\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"polars_data.parquet\" #replace with your file\n",
    "output_directory = \"split_files\"\n",
    "base_filename = \"data_chunk\"\n",
    "num_files_to_split = 5\n",
    "\n",
    "try:\n",
    "    df = pl.read_parquet(file_path)\n",
    "    split_polars_dataframe(df, num_files_to_split, output_directory, base_filename)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
