{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHSoKhqDg3LnepBUr5/rE9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":291,"metadata":{"id":"Al3mRrESBRef","executionInfo":{"status":"ok","timestamp":1745141217009,"user_tz":-480,"elapsed":37,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, precision_score, accuracy_score, recall_score, f1_score, r2_score, ndcg_score, roc_curve, auc, roc_auc_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from datetime import datetime as dt\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import LabelEncoder\n","import random\n","import numpy as np\n","import joblib\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/BT4301/Dataset/old dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THPAaIpdO6N-","executionInfo":{"status":"ok","timestamp":1745141219863,"user_tz":-480,"elapsed":2856,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}},"outputId":"3ac737cc-76f6-44c6-b5df-fba388a58ef3"},"execution_count":292,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/10UxIGmI1nRKsdfd_xARinm-FmI8_NeVG/BT4301/Dataset/old dataset\n"]}]},{"cell_type":"code","source":["class KindleDataset(Dataset):\n","    def __init__(self, customer_file='customer_clean.xlsx', product_file='product_clean.xlsx', seq_length=10, mode='train'):\n","        self.seq_length = seq_length\n","        self.mode = mode\n","\n","        self.data, self.label_encoders, self.title_tfidf = self.load_and_preprocess_data(customer_file, product_file)\n","\n","        self.user_sequences, self.targets = self.create_sequences_with_ratings()\n","\n","        if mode == 'eval':\n","            self.user_ids = self.data['user_id'].unique()\n","            self.user_sequences = self.user_sequences[[i for i, seq in enumerate(self.user_sequences) if len(seq) >= 2]]\n","            self.targets = self.targets[[i for i, seq in enumerate(self.user_sequences) if len(seq) >= 2]]\n","        else:\n","            self.user_ids = self.data['user_id'].unique()\n","\n","        self.num_books = len(self.label_encoders['parent_asin'].classes_)\n","        self.num_categories = len(self.label_encoders['categories'].classes_)\n","\n","    def load_and_preprocess_data(self, customer_file, product_file, min_purchases=5):\n","        review_data = pd.read_excel(customer_file)\n","        review_data = review_data.drop(columns=['title', 'verified_purchase', 'text', 'asin', 'helpful_vote'])\n","        metadata = pd.read_excel(product_file)\n","        metadata = metadata.drop(columns=['subtitle', 'language'])\n","        self.asin_to_category = dict(zip(review_data['parent_asin'], metadata['categories']))\n","        data = pd.merge(review_data, metadata, on=['parent_asin'])\n","\n","        # Filter users based on min_purchases\n","        user_counts = data.groupby('user_id')['parent_asin'].nunique()\n","        valid_users = user_counts[user_counts >= min_purchases].index\n","        data = data[data['user_id'].isin(valid_users)]\n","\n","        # Preprocessing steps\n","        data['rating'] = data['rating'].astype(float)\n","        data['price'] = pd.to_numeric(data['price'], errors='coerce')\n","        data['price'] = data['price'].fillna(0)\n","        data['price'] = data['price'].astype(float)\n","        data['print_length'] = data['print_length'].fillna(0)\n","        data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n","        data['publication_date'] = pd.to_datetime(data['publication_date'], errors='coerce')\n","        data['days_since_publication'] = (dt.now() - data['publication_date']).dt.days\n","        data['days_since_publication'].fillna(data['days_since_publication'].mean(), inplace=True)\n","\n","        # Encode categorical variables\n","        label_encoders = {}\n","        for col in ['user_id', 'parent_asin', 'categories', 'author']:\n","            label_encoders[col] = LabelEncoder()\n","            data[col] = label_encoders[col].fit_transform(data[col].fillna('Unknown'))\n","\n","        # Normalize numerical features\n","        self.scaler = MinMaxScaler()\n","        numeric_cols = ['rating', 'average_rating', 'rating_number', 'price', 'days_since_publication', 'print_length']\n","        data[numeric_cols] = self.scaler.fit_transform(data[numeric_cols])\n","\n","        # Apply TF-IDF on text data\n","        data['title'] = data['title'].fillna('Unknown')\n","        vectorizer_title = TfidfVectorizer(max_features=50)\n","        title_tfidf = vectorizer_title.fit_transform(data['title']).toarray()\n","\n","        return data, label_encoders, title_tfidf\n","\n","    def create_sequences_with_ratings(self):\n","        sequences = []\n","        targets = []\n","        numeric_cols = [column for column in self.data.columns.tolist()\n","                        if self.data[column].dtype in ('int64', 'float64')\n","                        and column not in ['rating', 'user_id', 'parent_asin']]\n","        groups = self.data.sort_values('timestamp').groupby('user_id')\n","        seq_length = self.seq_length\n","\n","        for user_id, group in groups:\n","            if len(group) >= self.seq_length:\n","                # Reset index to ensure it starts from 0\n","                group = group.reset_index(drop=True)\n","                for i in range(len(group) - seq_length + 1):\n","                    numeric_seq = group[numeric_cols].iloc[i:i + seq_length].values\n","                    title_seq = self.title_tfidf[group.index[i:i + seq_length]]\n","\n","                    # Ensure all categorical features are converted to numeric\n","                    parent_asin_seq = group['parent_asin'].iloc[i:i + seq_length].values.reshape(-1, 1).astype(np.float32)\n","                    categories_seq = group['categories'].iloc[i:i + seq_length].values.reshape(-1, 1).astype(np.float32)\n","                    author_seq = group['author'].iloc[i:i + seq_length].values.reshape(-1, 1).astype(np.float32)\n","\n","                    seq = np.concatenate([numeric_seq, parent_asin_seq, categories_seq,\n","                                          author_seq, title_seq], axis=1)\n","                    sequences.append(seq)\n","\n","                    # Target is the rating for the current book\n","                    target_val = group['rating'].iloc[i + seq_length - 1]\n","                    targets.append(target_val)\n","\n","        return torch.tensor(np.array(sequences), dtype=torch.float32), torch.tensor(np.array(targets), dtype=torch.float32)\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.user_sequences)\n","        else:\n","            return len(self.user_ids)\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","            user_sequence = self.user_sequences[idx]\n","            target = self.targets[idx]\n","        else:\n","            user_sequence = self.user_sequences[idx]\n","            target = self.targets[idx]\n","\n","        # Assign values to input_seq and labels\n","        input_seq = user_sequence\n","        labels = target\n","\n","        # Convert to PyTorch tensors\n","        input_seq = torch.tensor(input_seq, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.float32)\n","\n","        return input_seq, labels"],"metadata":{"id":"WdW3gJTWBWg2","executionInfo":{"status":"ok","timestamp":1745141219910,"user_tz":-480,"elapsed":50,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":293,"outputs":[]},{"cell_type":"code","source":["class LSTMRecommender(nn.Module):\n","    def __init__(self, num_books, embedding_dim, hidden_size, num_layers, input_size):\n","        super(LSTMRecommender, self).__init__()\n","        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_books)\n","\n","    def forward(self, x):\n","        num_features = x[:, :, :6]\n","        parent_asin = x[:, :, 6].long()\n","        categories = x[:, :, 7].long()\n","        author = x[:, :, 8].long()\n","        title_tfidf = x[:, :, 9:]\n","\n","        # Get embeddings for categorical features\n","        parent_asin_emb = self.book_embedding(parent_asin)\n","\n","        # Concatenate all features\n","        all_features = torch.cat([num_features, parent_asin_emb, title_tfidf,\n","                                  categories.unsqueeze(-1).float(), author.unsqueeze(-1).float()], dim=2)\n","\n","        input_size = all_features.shape[-1]\n","        self.lstm = nn.LSTM(input_size, self.lstm.hidden_size, self.lstm.num_layers, batch_first=True)\n","        lstm_out, _ = self.lstm(all_features)\n","\n","        # Get output for the last time step and predict for all books\n","        output = self.fc(lstm_out[:, -1, :])\n","\n","        # Reshape output if it's squeezed to a single value\n","        if output.dim() == 1:\n","            output = output.unsqueeze(0)\n","\n","        # If the model predicts the same rating for all books, `topk` will fail\n","        output = output + torch.randn_like(output) * 1e-6\n","\n","        # Apply sigmoid activation to ensure output is within a valid range\n","        output = torch.sigmoid(output)\n","\n","        return output"],"metadata":{"id":"3GpI14rODxN6","executionInfo":{"status":"ok","timestamp":1745141219927,"user_tz":-480,"elapsed":3,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":294,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, num_epochs=5, learning_rate=0.001, batch_size=64):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.MSELoss()\n","\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        num_batches = 0\n","\n","        for batch_X, batch_y in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(batch_X)\n","\n","            # Get predicted rating for the target book\n","            target_book_indices = batch_X[:, -1, 6].long()  # Get parent_asin from input\n","            predicted_ratings = outputs.gather(1, target_book_indices.unsqueeze(1)).squeeze(1)\n","\n","            loss = criterion(predicted_ratings, batch_y)  # Calculate loss for target book only\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            num_batches += 1\n","\n","        avg_loss = total_loss / num_batches\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")"],"metadata":{"id":"BeMPKH12D4Ma","executionInfo":{"status":"ok","timestamp":1745141219942,"user_tz":-480,"elapsed":14,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":295,"outputs":[]},{"cell_type":"code","source":["def predict(model, input_sequence, book_encoder, top_k=5):\n","    model.eval()\n","    with torch.no_grad():\n","\n","        input_sequence = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0) #\n","        predicted_ratings = model(input_sequence)\n","        _, top_k_indices = torch.topk(predicted_ratings.squeeze(), top_k)\n","        recommended_asins = book_encoder.inverse_transform(top_k_indices.cpu().numpy())\n","\n","    return list(map(str, recommended_asins))  # Return the recommended ASINs as strings"],"metadata":{"id":"Ss4rfG8-EM32","executionInfo":{"status":"ok","timestamp":1745141219986,"user_tz":-480,"elapsed":30,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":296,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, eval_loader, book_encoder, top_k=5):\n","    model.eval()\n","    hits = 0\n","    total = 0\n","    ndcg_sum = 0\n","\n","    with torch.no_grad():\n","        for input_sequence, target_rating in eval_loader:\n","            predicted_ratings = model(input_sequence)\n","            _, top_k_indices = torch.topk(predicted_ratings.squeeze(), top_k)\n","\n","            # Iterate over each sample in the batch\n","            for i in range(input_sequence.shape[0]):\n","                target_book_id = input_sequence[i, -1, -4].long().item()\n","\n","                if target_book_id in top_k_indices.cpu().numpy():\n","                    hits += 1\n","                    rank = np.where(top_k_indices.cpu().numpy() == target_book_id)[0][0] + 1\n","                    ndcg_sum += 1 / np.log2(rank + 1)\n","\n","            total += input_sequence.shape[0]\n","\n","    # Calculate overall metrics\n","    hit_rate = hits / total if total > 0 else 0\n","    ndcg = ndcg_sum / total if total > 0 else 0\n","\n","    print(f\"Evaluation Results!\")\n","    print(f\"Hits: {hits}/{total}\")\n","    print(f\"Hit Rate (Top {top_k}): {hit_rate:.4f}\")\n","    print(f\"NDCG (Top {top_k}): {ndcg:.4f}\")\n","\n","    return hit_rate, ndcg"],"metadata":{"id":"JQzhkuJJEfEm","executionInfo":{"status":"ok","timestamp":1745141220010,"user_tz":-480,"elapsed":23,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":297,"outputs":[]},{"cell_type":"code","source":["def main():\n","\n","    # Create train and eval datasets\n","    train_dataset = KindleDataset(mode='train')\n","    eval_dataset = KindleDataset(mode='eval')\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n","\n","    # Get necessary information from the dataset\n","    num_books = train_dataset.num_books\n","    num_categories = train_dataset.num_categories\n","    input_size = train_dataset.data.shape[1]\n","    embedding_dim = 64\n","    hidden_size = 64\n","    num_layers = 2\n","    num_epochs = 10\n","    learning_rate = 0.01\n","    batch_size = 64\n","\n","    # Instantiate the LSTMRecommender model\n","    model = LSTMRecommender(num_books, embedding_dim, hidden_size, num_layers, input_size)\n","\n","    # Train the model\n","    print(\"Training the model...\")\n","    train_model(model, train_loader)  # Use train_loader for training\n","\n","    # Evaluate the model\n","    print(\"\\nEvaluating the model...\")\n","    evaluate_model(model, eval_loader, train_dataset.label_encoders['parent_asin'], top_k=10)\n","\n","    # Save the model and components\n","    torch.save(model.state_dict(), 'lstm_model.pth')\n","    for encoder_name, encoder in train_dataset.label_encoders.items():\n","        joblib.dump(encoder, f'{encoder_name}_encoder.pkl')\n","    joblib.dump(train_dataset.title_tfidf, 'title_tfidf_vectorizer.pkl')\n","    joblib.dump(train_dataset.scaler, 'scaler.pkl')\n","    with open('asin_to_category.pkl', 'wb') as f:\n","        pickle.dump(train_dataset.asin_to_category, f)\n","    print(\"Model and components saved successfully!\")"],"metadata":{"id":"jSxI838xFR_W","executionInfo":{"status":"ok","timestamp":1745141220046,"user_tz":-480,"elapsed":28,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":298,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5BoJdsMFTIX","executionInfo":{"status":"ok","timestamp":1745141262276,"user_tz":-480,"elapsed":42228,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}},"outputId":"cc99cb12-2551-4978-d4f0-b0aa4b5da6e4"},"execution_count":299,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the model...\n","Epoch [1/5], Loss: 0.1491\n","Epoch [2/5], Loss: 0.1551\n","Epoch [3/5], Loss: 0.1560\n","Epoch [4/5], Loss: 0.1562\n","Epoch [5/5], Loss: 0.1544\n","\n","Evaluating the model...\n","Evaluation Results!\n","Hits: 0/32\n","Hit Rate (Top 10): 0.0000\n","NDCG (Top 10): 0.0000\n","Model and components saved successfully!\n"]}]},{"cell_type":"code","source":["# Load the trained model\n","train_dataset = KindleDataset(mode='train')\n","\n","num_books = train_dataset.num_books\n","num_categories = train_dataset.num_categories\n","input_size = train_dataset.data.shape[1]\n","embedding_dim = 64\n","hidden_size = 64\n","num_layers = 2\n","num_epochs = 10\n","learning_rate = 0.01\n","batch_size = 64\n","\n","# Instantiate the LSTMRecommender model\n","model = LSTMRecommender(num_books, embedding_dim, hidden_size, num_layers, input_size)\n","\n","model.eval()  # Set the model to evaluation mode\n","\n","# Load the book encoder\n","book_encoder = joblib.load('parent_asin_encoder.pkl')"],"metadata":{"id":"gc69xhIvZ02c","executionInfo":{"status":"ok","timestamp":1745141262300,"user_tz":-480,"elapsed":5,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":300,"outputs":[]},{"cell_type":"code","source":["eval_dataset = KindleDataset(mode='eval')\n","sample_input, _ = eval_dataset[0]"],"metadata":{"id":"4t2HbCVLarkB","executionInfo":{"status":"ok","timestamp":1745141262321,"user_tz":-480,"elapsed":19,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}}},"execution_count":301,"outputs":[]},{"cell_type":"code","source":["recommendations = predict(model, sample_input, book_encoder, top_k=5)\n","print(recommendations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdnK7xmRbUFr","executionInfo":{"status":"ok","timestamp":1745141262329,"user_tz":-480,"elapsed":5,"user":{"displayName":"Phùng Nguyên Hạnh","userId":"09363636402362848415"}},"outputId":"bb52fcd5-d343-4a44-97d1-88b84c8e49e5"},"execution_count":302,"outputs":[{"output_type":"stream","name":"stdout","text":["['B013P2ETBU', 'B003TZLMOG', 'B083RLXTLP', 'B08YGVGPFV', 'B09P38QMHM']\n"]}]}]}